{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import cvxpy as cp\n",
    "from numpy import linalg\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "import mosek\n",
    "\n",
    "\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import _scs_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CVXOPT', 'ECOS', 'GLPK', 'GLPK_MI', 'MOSEK', 'OSQP', 'SCS']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.installed_solvers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,202))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\", names=list(range(1,202)))\n",
    "df_test = pd.read_csv(\"test.csv\",names=list(range(1,202)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.65</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-1.58</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>1.31</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-1.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1     2     3     4     5     6     7     8     9     10   ...   192   193  \\\n",
       "0  0.0 -0.36 -0.91 -0.99 -0.57 -1.38 -1.54 -1.64  1.29  0.65  ...  1.15 -0.05   \n",
       "1  1.0 -1.40 -1.90  0.09  0.29 -0.30 -1.30  1.13 -2.38 -1.16  ...  0.48  0.24   \n",
       "2  1.0 -0.43  1.45 -0.68 -1.58  0.32 -0.14  0.23 -1.01 -0.39  ... -0.94  0.11   \n",
       "3  1.0 -0.76  0.30 -0.57 -0.33 -1.50  1.84  1.37  0.23  0.66  ... -0.42  0.06   \n",
       "4  0.0 -0.76  1.36  0.00 -1.44 -1.27 -0.76 -1.42 -0.58  0.11  ... -0.44  1.45   \n",
       "\n",
       "    194   195   196   197   198   199   200   201  \n",
       "0 -0.09  0.02  1.75  1.58  0.12  0.30  2.44 -1.26  \n",
       "1 -0.16 -0.48 -0.02 -0.35 -0.27 -0.20 -0.92 -0.46  \n",
       "2 -1.30 -0.24  0.74  0.88  1.37  0.12  0.01 -0.56  \n",
       "3 -1.05  0.35 -0.24 -0.69  1.31 -0.18 -1.54 -1.70  \n",
       "4 -1.18 -1.13 -0.14  0.04  0.33  1.20 -0.81 -1.16  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>8500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500118</td>\n",
       "      <td>0.015781</td>\n",
       "      <td>-0.013220</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.014908</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>-0.005094</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>-0.005624</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.025432</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>-0.010878</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.005359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500029</td>\n",
       "      <td>1.053792</td>\n",
       "      <td>1.057382</td>\n",
       "      <td>1.040262</td>\n",
       "      <td>1.050884</td>\n",
       "      <td>1.057882</td>\n",
       "      <td>1.039831</td>\n",
       "      <td>1.047432</td>\n",
       "      <td>1.048173</td>\n",
       "      <td>1.058878</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049972</td>\n",
       "      <td>1.051867</td>\n",
       "      <td>1.064075</td>\n",
       "      <td>1.042985</td>\n",
       "      <td>1.047585</td>\n",
       "      <td>1.041096</td>\n",
       "      <td>1.062399</td>\n",
       "      <td>1.036885</td>\n",
       "      <td>1.040154</td>\n",
       "      <td>1.051150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.980000</td>\n",
       "      <td>-4.720000</td>\n",
       "      <td>-3.570000</td>\n",
       "      <td>-4.160000</td>\n",
       "      <td>-4.230000</td>\n",
       "      <td>-4.600000</td>\n",
       "      <td>-3.560000</td>\n",
       "      <td>-3.870000</td>\n",
       "      <td>-3.720000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.140000</td>\n",
       "      <td>-4.340000</td>\n",
       "      <td>-4.040000</td>\n",
       "      <td>-3.820000</td>\n",
       "      <td>-3.480000</td>\n",
       "      <td>-3.930000</td>\n",
       "      <td>-5.120000</td>\n",
       "      <td>-3.620000</td>\n",
       "      <td>-4.080000</td>\n",
       "      <td>-4.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.680000</td>\n",
       "      <td>-0.730000</td>\n",
       "      <td>-0.692500</td>\n",
       "      <td>-0.710000</td>\n",
       "      <td>-0.690000</td>\n",
       "      <td>-0.690000</td>\n",
       "      <td>-0.710000</td>\n",
       "      <td>-0.710000</td>\n",
       "      <td>-0.720000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710000</td>\n",
       "      <td>-0.720000</td>\n",
       "      <td>-0.710000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.690000</td>\n",
       "      <td>-0.690000</td>\n",
       "      <td>-0.710000</td>\n",
       "      <td>-0.720000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.732500</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.280000</td>\n",
       "      <td>3.780000</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>3.770000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.180000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.210000</td>\n",
       "      <td>3.570000</td>\n",
       "      <td>4.110000</td>\n",
       "      <td>3.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1            2            3            4            5    \\\n",
       "count  8500.000000  8500.000000  8500.000000  8500.000000  8500.000000   \n",
       "mean      0.500118     0.015781    -0.013220     0.000401     0.007042   \n",
       "std       0.500029     1.053792     1.057382     1.040262     1.050884   \n",
       "min       0.000000    -3.980000    -4.720000    -3.570000    -4.160000   \n",
       "25%       0.000000    -0.680000    -0.730000    -0.692500    -0.710000   \n",
       "50%       1.000000     0.020000    -0.010000     0.010000     0.020000   \n",
       "75%       1.000000     0.730000     0.710000     0.720000     0.720000   \n",
       "max       1.000000     4.280000     3.780000     3.920000     3.770000   \n",
       "\n",
       "               6            7            8            9            10   ...  \\\n",
       "count  8500.000000  8500.000000  8500.000000  8500.000000  8500.000000  ...   \n",
       "mean      0.014908     0.007422     0.009867    -0.005094    -0.003275  ...   \n",
       "std       1.057882     1.039831     1.047432     1.048173     1.058878  ...   \n",
       "min      -4.230000    -4.600000    -3.560000    -3.870000    -3.720000  ...   \n",
       "25%      -0.690000    -0.690000    -0.710000    -0.710000    -0.720000  ...   \n",
       "50%       0.030000     0.000000     0.010000    -0.010000     0.000000  ...   \n",
       "75%       0.732500     0.730000     0.720000     0.720000     0.690000  ...   \n",
       "max       3.880000     3.950000     3.500000     4.020000     3.980000  ...   \n",
       "\n",
       "               192          193          194          195          196  \\\n",
       "count  8500.000000  8500.000000  8500.000000  8500.000000  8500.000000   \n",
       "mean      0.002894    -0.005624     0.004547     0.008327     0.025432   \n",
       "std       1.049972     1.051867     1.064075     1.042985     1.047585   \n",
       "min      -4.140000    -4.340000    -4.040000    -3.820000    -3.480000   \n",
       "25%      -0.710000    -0.720000    -0.710000    -0.700000    -0.690000   \n",
       "50%      -0.010000    -0.020000     0.000000     0.010000     0.030000   \n",
       "75%       0.702500     0.700000     0.740000     0.710000     0.720000   \n",
       "max       4.100000     4.180000     3.800000     4.250000     3.570000   \n",
       "\n",
       "               197          198          199          200          201  \n",
       "count  8500.000000  8500.000000  8500.000000  8500.000000  8500.000000  \n",
       "mean      0.004219     0.006253    -0.010878     0.002534     0.005359  \n",
       "std       1.041096     1.062399     1.036885     1.040154     1.051150  \n",
       "min      -3.930000    -5.120000    -3.620000    -4.080000    -4.260000  \n",
       "25%      -0.690000    -0.710000    -0.720000    -0.700000    -0.690000  \n",
       "50%       0.020000     0.005000     0.000000     0.010000     0.020000  \n",
       "75%       0.720000     0.720000     0.690000     0.710000     0.700000  \n",
       "max       3.750000     4.210000     3.570000     4.110000     3.930000  \n",
       "\n",
       "[8 rows x 201 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "x_train = df_train.drop(1, axis = 1)\n",
    "y_train = df_train[1].to_numpy()\n",
    "y_train = np.where(y_train == 0, -1, 1)\n",
    "\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "x_test = df_test.drop(1, axis = 1)\n",
    "y_test = df_test[1].to_numpy()\n",
    "y_test = np.where(y_test == 0, -1, 1)\n",
    "\n",
    "\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43825666, 0.44823529, 0.34445928, ..., 0.54520167, 0.7960928 ,\n",
       "        0.36630037],\n",
       "       [0.31234867, 0.33176471, 0.48865154, ..., 0.47566064, 0.38583639,\n",
       "        0.46398046],\n",
       "       [0.42978208, 0.72588235, 0.3858478 , ..., 0.5201669 , 0.4993895 ,\n",
       "        0.45177045],\n",
       "       ...,\n",
       "       [0.62469734, 0.64705882, 0.47396529, ..., 0.41585535, 0.53357753,\n",
       "        0.4029304 ],\n",
       "       [0.41525424, 0.69176471, 0.40186916, ..., 0.91655076, 0.17338217,\n",
       "        0.28815629],\n",
       "       [0.57869249, 0.44117647, 0.45660881, ..., 0.2837274 , 0.54090354,\n",
       "        0.64957265]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(x_train)\n",
    "x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_dual_soft (data_train , label_train , regularisation_para_C):\n",
    "    n_samples = len(data_train)\n",
    "    n = data_train.shape[1]\n",
    "    d = data_train.shape[0]\n",
    "\n",
    "    alpha = cp.Variable(shape=(d) , pos=True)\n",
    "    C = cp.Parameter()\n",
    "    C.value = regularisation_para_C\n",
    "    \n",
    "    H = np.dot((label_train[:,None] * data_train) , (  label_train[:,None] * data_train).T)\n",
    "\n",
    "   \n",
    "\n",
    "    obj = cp.Maximize(cp.sum(alpha)-(1/2)*cp.quad_form(alpha,H))\n",
    "\n",
    "    constraint_1 = [alpha >= 0]\n",
    "    constaint_2 = [alpha <= C/n]\n",
    "    constraint_3 = [(label_train@alpha) == 0]\n",
    "\n",
    "    constraint = constraint_1 + constaint_2 + constraint_3\n",
    "\n",
    "\n",
    "    prob = cp.Problem(obj, constraint)\n",
    "\n",
    "    results = prob.solve(solver=cp.MOSEK)\n",
    "    print (prob.status)\n",
    "    #print(alpha.value)\n",
    "    \n",
    "    \n",
    "    aaa = alpha.value\n",
    "    aaa = np.where(aaa>1e-5 , aaa, 0)\n",
    "    w_dual = ((label_train.T * aaa.T) @ data_train)\n",
    "    aaa = np.where(aaa>(regularisation_para_C/n)-0.01 , 0, aaa)\n",
    "    S = (aaa > 0).flatten()\n",
    "    b = label_train[S] - np.dot(data_train[S],w_dual)\n",
    "    \n",
    "    weight_w_bias = np.concatenate(([b[0]],w_dual))\n",
    "    \n",
    "    return weight_w_bias\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def svm_predict_dual(data_test , label_test , svm_model):\n",
    "    \n",
    "    predicted = []\n",
    "        \n",
    "    for x1 in data_test:\n",
    "            \n",
    "            results = np.where((np.dot(svm_model[1:], x1) + svm_model[0]) >= 0.0 , 1, -1)\n",
    "            predicted.append(results)\n",
    "    \n",
    "    \n",
    "    return accuracy_score(label_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_dual_hard (data_train , label_train):\n",
    "    n_samples = len(data_train)\n",
    "    n = data_train.shape[1]\n",
    "    d = data_train.shape[0]\n",
    "\n",
    "    alpha = cp.Variable(shape=(d),pos=True)\n",
    "#     C = cp.Parameter()\n",
    "#     C.value = regularisation_para_C\n",
    "    \n",
    "    H = np.dot((label_train[:,None] * data_train) , (  label_train[:,None] * data_train).T)\n",
    "\n",
    "   \n",
    "\n",
    "    obj = cp.Maximize(cp.sum(alpha)-(1/2)*cp.quad_form(alpha,H))\n",
    "\n",
    "    constraint_1 = [alpha >= 0]\n",
    "    #constaint_2 = [alpha <= C]\n",
    "    constraint_3 = [(label_train@alpha) == 0]\n",
    "\n",
    "#     constraint = constraint_1 + constaint_2 + constraint_3\n",
    "    constraint = constraint_1 + constraint_3\n",
    "\n",
    "\n",
    "    prob = cp.Problem(obj, constraint)\n",
    "\n",
    "    results = prob.solve(solver=cp.MOSEK)\n",
    "    print (prob.status)\n",
    "    #print(alpha.value)\n",
    "    \n",
    "    \n",
    "    aaa = alpha.value\n",
    "    aaa = np.where(aaa>1e-5 , aaa, 0)\n",
    "    w_dual = ((label_train.T * aaa.T) @ data_train)\n",
    "    #aaa = np.where(aaa>regularisation_para_C-0.1 , 0, aaa)\n",
    "    S = (aaa > 0).flatten()\n",
    "    b = label_train[S] - np.dot(data_train[S],w_dual)\n",
    "    \n",
    "    weight_w_bias = np.concatenate(([b[0]],w_dual))\n",
    "    \n",
    "    return weight_w_bias\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def svm_predict_dual(data_test , label_test , svm_model):\n",
    "    \n",
    "    predicted = []\n",
    "        \n",
    "    for x1 in data_test:\n",
    "            \n",
    "            results = np.where((np.dot(svm_model[1:], x1) + svm_model[0]) >= 0.0 , 1, -1)\n",
    "            predicted.append(results)\n",
    "    \n",
    "    \n",
    "    return accuracy_score(label_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_primal_hard (data_train, label_train):\n",
    "    n_samples = len(data_train)\n",
    "    d = data_train.shape[1]\n",
    "    n = data_train.shape[0]\n",
    "\n",
    "    W = cp.Variable((d))\n",
    "    bias = cp.Variable()\n",
    "\n",
    "    obj = cp.Minimize(1/2*cp.norm(W,2))\n",
    "\n",
    "\n",
    "    constranit_1 = [cp.multiply(label_train,(data_train@W+bias)) >=1]\n",
    "\n",
    "\n",
    "    constraints = constranit_1 \n",
    "\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "    print (prob.status)\n",
    "\n",
    "    w = W.value\n",
    "    b = bias.value\n",
    "    \n",
    "    weight_w_bias = np.concatenate(([b],w))\n",
    "    \n",
    "    return weight_w_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_primal_soft (data_train, label_train, regularisation_para_C):\n",
    "    n_samples = len(data_train)\n",
    "    n = data_train.shape[0]\n",
    "    d = data_train.shape[1]\n",
    "\n",
    "    W = cp.Variable((d))\n",
    "    bias = cp.Variable()\n",
    "    epi = cp.Variable(n_samples)\n",
    "    C = cp.Parameter()\n",
    "    C.value = regularisation_para_C\n",
    "    a = cp.Constant(n_samples)\n",
    "    obj = cp.Minimize(1/2*cp.norm(W,2) + (C/n_samples)*cp.sum(epi))\n",
    "\n",
    "    constranit_1 = [cp.multiply(label_train,(data_train@W+bias)) >= 1-epi]\n",
    "\n",
    "    constraint_2 = [epi >= 0]\n",
    "\n",
    "    constraints = constranit_1 + constraint_2\n",
    "\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "    print (prob.status)\n",
    "\n",
    "    w = W.value\n",
    "    b = bias.value\n",
    "    ee = epi.value\n",
    "    \n",
    "    weight_w_bias = np.concatenate(([b],w))\n",
    "    \n",
    "    return weight_w_bias\n",
    "\n",
    "    \n",
    "\n",
    "def svm_predict_primal(data_test , label_test , svm_model):\n",
    "    \n",
    "    \n",
    "    predicted = []\n",
    "        \n",
    "    for x1 in data_test:\n",
    "            \n",
    "            results = np.where((np.dot(svm_model[1:], x1) + svm_model[0]) >= 0.0 , 1, -1)\n",
    "            predicted.append(results)\n",
    "    \n",
    "    \n",
    "    return accuracy_score(label_test, predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class, y_class = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "y_class = np.where(y_class == 0, -1, 1)\n",
    "\n",
    "\n",
    "X_circles, y_circles = make_circles(n_samples=100, noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "y_circles = np.where(y_circles == 0, -1, 1)\n",
    "\n",
    "\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=100, noise=0.3, random_state=0)\n",
    "\n",
    "y_moons = np.where(y_moons == 0, -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal\n",
      "the accuracy of the Primal hard margin model is : 1.0\n",
      "the bias term is  0.0038148465464497186  the weights are  [-1.75451608  0.0777831 ]\n"
     ]
    }
   ],
   "source": [
    "# separable case\n",
    "SVM_model_Primal_hard_test = svm_train_primal_hard(X_class, y_class)\n",
    "\n",
    "test_accyracy_Primal_Hard_test = svm_predict_primal(X_class,y_class, SVM_model_Primal_hard_test)\n",
    "\n",
    "print(\"the accuracy of the Primal hard margin model is :\", test_accyracy_Primal_Hard_test)\n",
    "print(\"the bias term is \", SVM_model_Primal_hard_test[0], \" the weights are \", SVM_model_Primal_hard_test[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal\n",
      "the accuracy of the Dual hard margin model is : 1.0\n",
      "the bias term is  0.003771064875461083  the weights are  [-1.7545558  0.0777976]\n"
     ]
    }
   ],
   "source": [
    "SVM_model_Dual_Hard_test = svm_train_dual_hard(X_class , y_class)\n",
    "test_accuracy_Dual_Hard_test = svm_predict_dual(X_class , y_class , SVM_model_Dual_Hard_test)\n",
    "print(\"the accuracy of the Dual hard margin model is :\", test_accuracy_Dual_Hard_test)\n",
    "print(\"the bias term is \", SVM_model_Dual_Hard_test[0], \" the weights are \", SVM_model_Dual_Hard_test[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [[-1.74054548  0.07716381]]\n",
      "b =  [0.01177817]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_hard_test = SVC(C = 1, kernel = 'linear', tol=0.0001,  max_iter=-1)\n",
    "clf_hard_test.fit(X_class, y_class) \n",
    "print('w = ',clf_hard_test.coef_)\n",
    "print('b = ',clf_hard_test.intercept_)\n",
    "predicted_sklearn_hard_test = clf_hard_test.predict(X_class)\n",
    "\n",
    "accuracy_score(y_class, predicted_sklearn_hard_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal\n",
      "the accuracy of the Dual soft margin model is : 0.59\n",
      "the bias term is  0.29015581075361685  the weights are  [ 0.7589323  -0.65157069]\n"
     ]
    }
   ],
   "source": [
    "SVM_model_Dual_Soft_test = svm_train_dual_soft(X_circles , y_circles,130)\n",
    "test_accuracy_Dual_Soft_test = svm_predict_dual(X_circles , y_circles , SVM_model_Dual_Soft_test)\n",
    "print(\"the accuracy of the Dual soft margin model is :\", test_accuracy_Dual_Soft_test)\n",
    "print(\"the bias term is \", SVM_model_Dual_Soft_test[0], \" the weights are \", SVM_model_Dual_Soft_test[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal\n",
      "the accuracy of the Primal soft margin model is : 0.59\n",
      "the bias term is  0.29023341561837973  the weights are  [ 0.7588472  -0.65150936]\n"
     ]
    }
   ],
   "source": [
    "SVM_model_Primal_Soft_test = svm_train_primal_soft(X_circles , y_circles,130)\n",
    "test_accuracy_Primal_Soft_test = svm_predict_dual(X_circles , y_circles , SVM_model_Primal_Soft_test)\n",
    "print(\"the accuracy of the Primal soft margin model is :\", test_accuracy_Primal_Soft_test)\n",
    "print(\"the bias term is \", SVM_model_Primal_Soft_test[0], \" the weights are \", SVM_model_Primal_Soft_test[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [[ 0.75886286 -0.65153027]]\n",
      "b =  [0.29022996]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_soft_test = SVC(C = 130, kernel = 'linear', tol=0.0001,  max_iter=-1)\n",
    "clf_soft_test.fit(X_circles, y_circles) \n",
    "print('w = ',clf_soft_test.coef_)\n",
    "print('b = ',clf_soft_test.intercept_)\n",
    "predicted_sklearn_soft_test = clf_soft_test.predict(X_circles)\n",
    "\n",
    "accuracy_score(y_circles, predicted_sklearn_soft_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal\n",
      "the accuracy of the Dual soft margin model on testing set is : 0.968\n",
      "the accuracy of the Dual soft margin model on training set is : 0.9774117647058823\n"
     ]
    }
   ],
   "source": [
    "SVM_model_Dual_Soft_data = svm_train_dual_soft(x_train , y_train,1000)\n",
    "test_accuracy_Dual_Soft_data_test_set = svm_predict_dual(x_test , y_test , SVM_model_Dual_Soft_data)\n",
    "test_accuracy_Dual_Soft_data_train_set = svm_predict_dual(x_train , y_train , SVM_model_Dual_Soft_data)\n",
    "print(\"the accuracy of the Dual soft margin model on testing set is :\", test_accuracy_Dual_Soft_data_test_set)\n",
    "print(\"the accuracy of the Dual soft margin model on training set is :\", test_accuracy_Dual_Soft_data_train_set)\n",
    "\n",
    "#print(\"the bias term is \", test_accuracy_Dual_Soft_data[0], \" the weights are \", test_accuracy_Dual_Soft_data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal\n",
      "the accuracy of the Primal soft margin model on testing set is: 0.968\n",
      "the accuracy of the Primal soft margin model on training set is: 0.9771764705882353\n"
     ]
    }
   ],
   "source": [
    "SVM_model_Primal_Soft_data = svm_train_primal_soft(x_train , y_train,1000)\n",
    "test_accuracy_Primal_Soft_data_testing_set = svm_predict_dual(x_test , y_test , SVM_model_Primal_Soft_data)\n",
    "test_accuracy_Primal_Soft_data_training_set = svm_predict_dual(x_train , y_train , SVM_model_Primal_Soft_data)\n",
    "print(\"the accuracy of the Primal soft margin model on testing set is:\", test_accuracy_Primal_Soft_data_testing_set)\n",
    "print(\"the accuracy of the Primal soft margin model on training set is:\", test_accuracy_Primal_Soft_data_training_set)\n",
    "\n",
    "#print(\"the bias term is \", test_accuracy_Primal_Soft_data[0], \" the weights are \", test_accuracy_Primal_Soft_data[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775294117647059"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_soft_data = SVC(C = 1000, kernel = 'linear', tol=0.0001,  max_iter=-1)\n",
    "clf_soft_data.fit(x_train, y_train) \n",
    "#print('w = ',clf_soft_data.coef_)\n",
    "#print('b = ',clf_soft_data.intercept_)\n",
    "predicted_sklearn_soft_data_test_set = clf_soft_data.predict(x_test)\n",
    "predicted_sklearn_soft_data_trainig_set = clf_soft_data.predict(x_train)\n",
    "\n",
    "\n",
    "accuracy_score(y_test, predicted_sklearn_soft_data_test_set)\n",
    "accuracy_score(y_train, predicted_sklearn_soft_data_trainig_set )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673333333333334"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predicted_sklearn_soft_data_test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775294117647059"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, predicted_sklearn_soft_data_trainig_set )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-separable case\n",
    "\n",
    "# SVM_Model_Primal_Soft  = svm_train_primal_soft(x_train, y_train,100)\n",
    "\n",
    "# test_accuracy_Primal_Soft = svm_predict_primal(x_test,y_test, SVM_Model_Primal_Soft)\n",
    "\n",
    "# print(\"the accuracy of the Primal soft margin model is :\", test_accuracy_Primal_Soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"the bias term is \", SVM_Model_Primal_Soft[0], \" the weights are \", SVM_Model_Primal_Soft[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# svm_model_p_s = svm_train_primal_soft(x_train , y_train ,1200)\n",
    "\n",
    "# svm_model_p_h = svm_train_primal_hard(x_train , y_train)\n",
    "\n",
    "# SVM_model_Dual_Soft = svm_train_dual_soft(x_train , y_train ,100)\n",
    "\n",
    "# test_accuracy_primal_soft = svm_predict_primal(x_test , y_test , svm_model_p_s)\n",
    "\n",
    "# test_accuracy_primal_hard = svm_predict_primal(x_test , y_test , svm_model_p_h)\n",
    "\n",
    "\n",
    "\n",
    "# print(test_accuracy_primal_soft)\n",
    "# print(test_accuracy_primal_hard)\n",
    "# # print(classification_report(y_test, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_accuracy_dual_soft = svm_predict_dual(x_test , y_test , SVM_model_Dual_Soft)\n",
    "# print(test_accuracy_dual_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# svm_model_d_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_accuracy_Dual_Hard = svm_predict_dual(x_test , y_test , SVM_model_Dual_Hard)\n",
    "# print(test_accuracy_Dual_Hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# clf = SVC(C = 1200, kernel = 'linear')\n",
    "# clf.fit(x_train, y_train) \n",
    "# predicted_svm_sk = clf.predict(x_test)\n",
    "# # print('w = ',clf.coef_)\n",
    "# # print('b = ',clf.intercept_)\n",
    "# # print('Indices of support vectors = ', clf.support_)\n",
    "# # print('Support vectors = ', clf.support_vectors_)\n",
    "# # print('Number of support vectors for each class = ', clf.n_support_)\n",
    "# # print('Coefficients of the support vector in the decision function = ', np.abs(clf.dual_coef_))\n",
    "# print(accuracy_score(y_test, predicted_svm_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
