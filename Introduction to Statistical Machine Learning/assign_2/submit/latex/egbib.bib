@article{Minsky1969,
abstract = {Perceptrons - the first systematic study of parallelism in computation - has remained a classical work on threshold automata networks for nearly two decades.},
author = {Minsky, Marvin and Papert, Seymour},
doi = {10.1016/S0019-9958(70)90409-2},
file = {:home/roasted{\_}kernel/uni{\_}subjects/Deep Learning Fundamentals/ref/block1970.pdf:pdf},
isbn = {0262631113},
issn = {00199958},
journal = {MIT Press Cambridge MA},
pages = {20},
title = {{Perceptrons: expanded edition}},
url = {http://mitpress.mit.edu/book-home.tcl?isbn=0262631113},
volume = {522},
year = {1969}
}
@article{Murphy2017,
abstract = {Frank Rosenblatt invented the perceptron algorithm in 1957 as part of an early attempt to build "brain models", artificial neural networks. In this paper, we apply tools from symbolic logic such as dependent type theory as implemented in Coq to build, and prove convergence of, one-layer perceptrons (specifically, we show that our Coq implementation converges to a binary classifier when trained on linearly separable datasets). Our perceptron and proof are extensible, which we demonstrate by adapting our convergence proof to the averaged perceptron, a common variant of the basic perceptron algorithm. We perform experiments to evaluate the performance of our Coq perceptron vs. an arbitrary-precision C++ implementation and against a hybrid implementation in which separators learned in C++ are certified in Coq. We find that by carefully optimizing the extraction of our Coq perceptron, we can meet - and occasionally exceed - the performance of the arbitrary-precision C++ implementation. Our hybrid Coq certifier demonstrates an architecture for building high-assurance machine-learning systems that reuse existing codebases.},
author = {Murphy, Charlie and Gray, Patrick and Stewart, Gordon},
doi = {10.1145/3088525.3088673},
file = {:home/roasted{\_}kernel/uni{\_}subjects/Deep Learning Fundamentals/ref/mapl{\_}2017.pdf:pdf},
isbn = {9781450350716},
journal = {MAPL 2017 - Proceedings of the 1st ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, co-located with PLDI 2017},
keywords = {Convergence,Interactive theorem proving,Linear classification,Perceptron},
pages = {43--50},
title = {{Verified perceptron convergence theorem}},
year = {2017}
}
@article{Cover1965,
author = {Cover, M},
file = {:home/roasted{\_}kernel/uni{\_}subjects/Deep Learning Fundamentals/ref/cover1965.pdf:pdf},
pages = {326--334},
title = {{Inequalities Applications Pattern}},
year = {1965}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
file = {:home/roasted{\_}kernel/uni{\_}subjects/Deep Learning Fundamentals/ref/10.1.1.335.3398.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATION, STORAGE, MODEL FOR, IN BRAIN BRAIN, INFORMATION STORAGE IN, MODEL FOR LEARNING {\&} MEMORY},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
