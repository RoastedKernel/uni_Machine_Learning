\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amssymb}
%\usepackage{hyperref}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Assignment 1: Predict diabetes via Perceptron}

\author{Moaz Mohamed\\
The University of Adelaide\\
Adelaide SA 5005\\
{\tt\small a1779177@student.adelaide.edu.au}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both

}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
%\begin{abstract}
  % The ABSTRACT is to be in fully-justified italicized text, at the top
  % of the left-hand column, below the author and affiliation
  % information. Use the word ``Abstract'' as the title, in 12-point
  % Times, boldface type, centered relative to the column, initially
  % capitalized. The abstract is to be in 10-point, single-spaced type.
  % Leave two blank lines after the Abstract, then begin the main text.
 %  Look at previous CVPR abstracts to get a feel for style and length.
%\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Over the last few years, deep learning had a significant impact in the medical field. 
The application of numerous Deep learning algorithms have been implemented in different sector of the medical field. In this paper Perceptron algorithm will be explored and its performance will be investigated on classification task to predict diabetes on the provided dataset.In addition to a complete review of the Perceptron algorithm describing its strength and weakness. An alternative algorithm will be introduced and explored that can mitigate some of the inherit weakness of Perceptron. 
%-------------------------------------------------------------------------
\section{Methodology and Background}



\subsection{Support Vector Machines}

An alternative approach to pattern recognition is using Support Vector Machines(SVM). Which in hindsight is very similar to the perceptron algorithm but SVM ensures finding a hyperplane  that can separate between the two classes with maximum margin. the decision for either positive or negative classification follows the eq \ref{eq:dbsvm}. 


\begin{equation}
y_{i}\left(w x_{i}+b\right) \geq 1
\label{eq:dbsvm}
\end{equation}

for the data points that lie exactly on the support vector boundary. 
\begin{equation}
y_{i}\left(w x_{i}+b\right)-1=0
\label{eq:svm-1}
\end{equation}

finding the width for the maximum margin will depend on finding the distance between the negative and positive points that exist on the boundary $\left(x_{p}-x_{n}\right)$ multiplying by $\frac{\bar{w}}{\|w\|}$ as unit vector provides the width. 

\begin{equation}
\left(x_{p}-x_{n}\right) \times \frac{\bar{w}}{\|w\|}
\end{equation}

using eq \ref{eq:svm-1} the term that need to be maximized comes naturally as $\frac{2}{\|w\|}$ or for minimization as $\frac{1}{2} \times\|w\|^{2}$

\begin{equation}
\begin{array}{l}
\frac{1}{\|w\|} \times\left(x_{p} w-x_{n} w\right) \\
\frac{1}{\|w\|} \times([1-b]+[1+b])
\end{array}
\end{equation}


\begin{equation}
\begin{array}{l}
\frac{2}{\|w\|} \\ \\
\frac{1}{2} \times\|w\|^{2}
\end{array}
\end{equation}


thus the margin is 

\begin{equation}
\frac{1}{2} \times\|w\|^{2}
\end{equation}


which can be constructed as a function to 

minimize 
\begin{equation}
j\left( w\right) =\dfrac{1}{2}\times \left\| w\right\| ^{2}
\end{equation}

subject to 
\begin{equation}
y_{i}\left( w^{T}x_{i}+b\right) \geq 1 , \forall j
\end{equation}

Such formulation does work on finding the optimal hyperplane and it can be solved with  optimization tool such as CVXOPT or MOSER. a slack variable can also be added to relax the SVM with $C$ as regularization parameter. 

minimization of 

\begin{equation}
\frac{1}{2} \times\|w\|^{2}+c\times \dfrac{1}{n}\sum ^{n}_{i=1}\xi _{i}
\end{equation}

subject to 

\begin{equation}
\begin{aligned}y_{i}\left( w^{T}x_{i}+b\right) \geq 1-\xi i\\
\xi _{i}\geq 0,\forall n\end{aligned}
\end{equation}


Kuhn-Tucker theorem and Lagrange multipliers can be utilized to drive the previous problem from primal problem to dual problem. 

\begin{equation}
L_{D}\left( \alpha \right) =\sum ^{n}_{i}\alpha _{i}-\dfrac{1}{2}\sum ^{n}_{i=1}\sum ^{n}_{j=1}\alpha _{i}\alpha _{j}y_{i}y_{j} <X_{i}^{T}X_{j} >
\label{eqn:l}
\end{equation}

subject to for (non-separable case)
\begin{equation}
0 \leq \alpha_{i} \leq C/n , \forall i,\sum ^{n}_{i=1}\alpha _{i}y_{i}=0
\end{equation}

or subject to for (separable case)

\begin{equation}
\alpha_{i} \geq 0 , \forall i,\sum ^{n}_{i=1}\alpha _{i}y_{i}=0
\end{equation}

and the $\overrightarrow{w}$ and the bias term can be calculated as follows. 
\begin{equation}
w=\sum ^{n}_{i=1}\alpha _{i}y_{j}x_{i}
\label{eqn:weight}
\end{equation}

\begin{equation}
b =\dfrac{1}{y_{j}}-w^{T}x_{j}
\end{equation}
 
It is interesting to note that in equation \ref{eqn:weight} the weight vector can be formulated as summation of alpha, label and the support vectors. Hence the number of the support vectors is depended on $\alpha _{i}$ as some values of $\alpha _{i}$ will be zero.
instead of in the primal problem where the $\overrightarrow{w}$ is obtained directly from solving the primal optimization problem. Such difference allows for the usage for something called the "Kernel Trick".
 
\begin{equation}
L_{D}\left( \alpha \right) =\sum ^{n}_{i=1}\alpha _{i}-\dfrac{1}{2}\sum ^{n}_{i=1}\sum ^{n}_{j=1}\alpha _{i}\alpha _{j}y_{i}y_{j}K\left( x_{j},x_{i}\right) 
\label{eqn:lk}
\end{equation} 


\begin{figure}[h!]
  \includegraphics[width=\linewidth]{kernel.png}
  \caption{Mapping}
  \label{fig:kernel}
\end{figure}


\begin{equation}
K\left( x_{j},x_{i}\right) =\left( x_{i}^{T}x_{j}+1\right) ^{p}
\label{eq:polyk}
\end{equation}


\begin{equation}
K\left( x_{i},x_{j}\right) =\exp \left( -\dfrac{1}{2\sigma ^{2}}\times \left\| x_{j}-x_{i}\right\| ^{2}\right) 
\label{eq:rbfk}
\end{equation}

The optimization problems for both equations \ref{eqn:lk} and \ref{eqn:l} except for the $( x_{j},x_{i})$ in equation \ref{eqn:lk} is being applied to a kernel (Ex eq \ref{eq:polyk} and \ref{eq:rbfk}) and the data is being projected into higher dimensions. here the SVM is exploiting cover's theorem."A complex pattern-classification problem, cast in a high-dimensional space non-linearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated." \cite{Cover1965} Which increases the complexity of the SVM by applying the kernel and enabling it to classify classes even in non-separable cases as demonstrated in figure \ref{fig:kernel} and \ref{fig:svm_exp_k}

\section{Experimental Analysis.}

\subsection{SVM Primal and Dual}
By utilizing the support vectors in eq \ref{eqn:weight} SVM is able to find the support vectors that can maximize the margin between the two classes as demonstrated in equation \ref{eqn:l} but SVM without utilizing a kernel SVM has the same weakness as Perceptron in that regard. But when a kernel is utilized project the data into higher dimensions SVM is capable of finding a hyperplane between the two classes as demonstrated in figure \ref{fig:svm_exp_k}.

\begin{figure}[htb]
  \includegraphics[width=\linewidth]{svm_lin_non_sep.png}
  \caption{SVM decision boundary in non-linearly separable data}
  \label{fig:svm_exp_non_lin}
\end{figure}

\vspace{3.00mm} 

\begin{figure}[htb]
  \includegraphics[width=\linewidth]{svm_lin_sep.png}
  \caption{SVM decision boundary in linearly separable data}
  \label{fig:svm_exp_lin}
\end{figure}

\vspace{50.00mm} 

\begin{figure}[htb]
  \includegraphics[width=\linewidth]{svm_k_non.png}
  \caption{SVM with polynomial kernel applied.Decision boundary in non-linearly separable data}
  \label{fig:svm_exp_k}
\end{figure}





\section{Performance on diabetes data-set}
In table \ref{tab:res} are the results. with aggressive hyper-parameter tuning all algorithms did perform with no apparent close contender. the diabetes data-set is full of missing values and noise with careful data cleaning steps i expect better results. 



\begin{table}[htb]
\centering
\begin{tabular}{|l|l|l|l|} 
\toprule
Algorithm   & W                           & bias       & accuracy  \\ 
\hline
Dual Soft   & {[} 0.7589323, -0.65157069] & 0.29015581 & 59\%      \\ 
\hline
Primal Soft & {[} 0.7588472, -0.65150936] & 0.29023341 & 59\%      \\ 
\hline
sklearn     & {[} 0.7588628, -0.65153027] & 0.29022996 & 59\%      \\
\bottomrule
\end{tabular}
\caption{SVM implementation for Soft margin}
\label{tab:SVM_soft}
\end{table}



\begin{table}[htb]
\centering
\begin{tabular}{|l|l|l|l|} 
\toprule
Algorithm   & W                        & bias      & accuracy  \\ 
\hline
Dual Hard   & {[}-1.754555, 0.0777976] & 0.0037710 & 100\%     \\ 
\hline
Primal Hard & {[}-1.754516, 0.077783]  & 0.0038148 & 100\%     \\ 
\hline
sklearn     & {[}-1.740545, 0.077163]  & 0.0117781 & 100\%     \\
\bottomrule
\end{tabular}
\caption{SVM implementation for Hard margin}
\label{tab:SVM_hard}
\end{table}



\begin{table}
\centering
\begin{tabular}{|l|l|l|l|} 
\toprule
Algorithm   & training set & testing set & C     \\ 
\hline
Dual Soft   & 97.74\%      & 96.8\%      & 1000  \\ 
\hline
Primal Soft & 97.71\%      & 96.8\%      & 1000  \\ 
\hline
sklearn     & 97.75\%      & 96.7\%      & 1000  \\
\bottomrule
\end{tabular}
\caption{SVM implementation for Soft margin on provided dataset}
\label{tab:SVM_soft_data}
\end{table}

\section{Conclusion}
Perceptron and different variations of Support vector machines algorithm have been explored and analyzed. mathematical formulation of all the algorithms have been studied. decision boundaries for have been made for a careful study of the behavior of various algorithms in different data situations. Advantageous and weakness have been identified for all the explored algorithms. Perceptron and its creator have created and paved the way for the emergence of the field of machine learning. all subsequent algorithms builds on top of the weakness of the previous work as seen in perceptron, SVM and later kernalised SVM.
\section{Code}

all the code for this assignment is provided in this GitHub repo 
\url{https://github.com/RoastedKernel/uni_Machine_Learning}
\bibliography{egbib.bib} 
\bibliographystyle{ieeetr}

\end{document}
